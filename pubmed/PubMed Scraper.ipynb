{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# THIS FILE IS OBSOLETE #\n",
    "#########################\n",
    "\n",
    "# No more major PubMed scraper modifications need to be made, thus this file is no\n",
    "# longer needed and obsolete. Please see the repository \"PubMedModifications\" branch\n",
    "# to view detailed changes.\n",
    "# https://github.com/davidokao/StudyFind-0320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_researcher(name, email=None, org=None, study_num=5, search_factor=1.0, doi=None, pmid=None):\n",
    "    \"\"\"\n",
    "    Generates a researcher dictionary given a name and other identifying information.\n",
    "    \n",
    "    This method is broken into two sections: Link generation and researcher generation. The link\n",
    "    created contains results for the first study_num * search_factor results based off of a pmid\n",
    "    that is either generated or given that should contain the researcher searched.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher in the form \"First Last\"\n",
    "    email -- the email of the researcher (default 'None')\n",
    "    org -- the organization of the researcher (default 'None')\n",
    "    study_num -- the requested number of studies returned (default 5)\n",
    "    search_factor -- the search range for the studies pulled (default 1.0)\n",
    "    doi -- the DOI name of an existing publication by this researcher (default None)\n",
    "    pmid -- the PubMed ID of an existing article by this researcher (default None)\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Link Generation #\n",
    "    ###################\n",
    "    \n",
    "    # Search Link Creation\n",
    "    if doi and not pmid:\n",
    "        pmid = doi_to_pmid(doi)\n",
    "        time.sleep(0.1) # UrlLib requires sleep time between queries, which occurs here and below\n",
    "    elif not pmid:\n",
    "        pmid = ft_query(name, email, org)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    \n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    query_num = int(study_num * search_factor)\n",
    "    \n",
    "    url = base + \"&retmax=\" + str(query_num) + \"&term=\" + query + \"&cauthor_id=\" + pmid\n",
    "    \n",
    "    # Create List of Articles\n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page = json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    #########################\n",
    "    # Researcher Generation #\n",
    "    #########################\n",
    "    \n",
    "    researcher = {'name': name, 'email': 'None', 'organization': 'None', 'topics': 'None', 'pmid': \"None\",'studies': 'None'}\n",
    "    articles = []\n",
    "    topics = []\n",
    "    api_request_counter = 0\n",
    "    \n",
    "    for pubmed_id in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=pubmed_id\"\n",
    "        url = url.replace('pubmed_id', pubmed_id)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        article, tags = generate_article_and_tags(soup)\n",
    "        articles.append(article)\n",
    "        topics = topics + tags\n",
    "        \n",
    "        # The following code removed articles that had no information in\n",
    "        # its entries. We believe the problem was due to an api request error\n",
    "        # as we were not giving it enough time between queries.\n",
    "        \n",
    "        #add = False\n",
    "        #for entries in article.values():\n",
    "        #    if entries != 'None':\n",
    "        #        add = True\n",
    "        #if add:\n",
    "        #    articles.append(article)\n",
    "        #    topics = topics + tags\n",
    "        #else:\n",
    "        #    raise Exception(\"Blank study encountered. Please save query information for further replication.\")\n",
    "        \n",
    "        api_request_counter += 1\n",
    "        if api_request_counter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep inbetween.\n",
    "            api_request_counter = 0\n",
    "    \n",
    "    if email:\n",
    "        researcher['email'] = email\n",
    "    if org:\n",
    "        researcher['organization'] = org\n",
    "    if pmid:\n",
    "        researcher['pmid'] = pmid # There should always be a pmid returned.\n",
    "    if articles:\n",
    "        articles = sorted(articles, key = lambda i: i['publication date'], reverse=True)\n",
    "        researcher['studies'] = articles[:study_num]\n",
    "    if topics:\n",
    "        researcher['topics'] = reduce_tags(topics)\n",
    "    if pmid:\n",
    "        researcher['pmid'] = pmid # There should always be a pmid returned.\n",
    "    \n",
    "    return researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_to_pmid(doi):\n",
    "    \"\"\"\n",
    "    Returns a valid PubMed ID if it exists.\n",
    "    \n",
    "    Given a valid DOI name or the None object, use the NCBI id converter API to potentially produce a PubMed ID.\n",
    "    If no results are obtained, the None object should be returned.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    doi -- the DOI name of the publication to be searched\n",
    "    \"\"\"\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids=\" + doi + \"&format=json\"\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page = json.loads(webpage)\n",
    "    pmid = dict_page[\"records\"][0][\"pmid\"]\n",
    "    \n",
    "    return pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_query(name, email=None, org=None, query_num=10):\n",
    "    \"\"\"\n",
    "    Searches for verifiable articles for a particular researcher, with the prior information\n",
    "    that this is the first-time query being performed.\n",
    "    \n",
    "    If we have not previously searched for this researcher, this method should run a search of the first\n",
    "    query_num articles and pick the one with the most verifiable information. The name of this function\n",
    "    is shorthand for \"first-time query\".\n",
    "    \n",
    "    The logic can be changed to determine which intermediate pmid is selected,\n",
    "    but keep in mind only one should be selected.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    query_num -- the number of articles to search through (default 10)\n",
    "    \"\"\"\n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    \n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    \n",
    "    url = base + \"&retmax=\" + str(query_num) + \"&term=\" + query\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page = json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    pubmed_id_list = []\n",
    "    email_counter = 0\n",
    "    org_counter = 0\n",
    "    api_request_counter = 0\n",
    "    \n",
    "    for pubmed_id in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=pubmed_id\"\n",
    "        url = url.replace('pubmed_id', pubmed_id)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        vtable = ft_verify(soup, name, email, org)\n",
    "        if vtable[0]: # If the email was verified\n",
    "            pubmed_id_list.insert(email_counter, pubmed_id)\n",
    "            email_counter += 1\n",
    "        elif vtable[1]: # If the organization was verified\n",
    "            pubmed_id_list.insert(email_counter + org_counter, pubmed_id)\n",
    "            org_counter += 1\n",
    "        else:\n",
    "            pubmed_id_list.append(pubmed_id)\n",
    "        \n",
    "        api_request_counter += 1\n",
    "        if api_request_counter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep in between.\n",
    "            api_request_counter = 0\n",
    "    \n",
    "    return pubmed_id_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_verify(soup, name, email=None, org=None):\n",
    "    \"\"\"\n",
    "    Verifies information in a given PubMed article, with the prior information\n",
    "    that this is within the first-time query being performed.\n",
    "    \n",
    "    Understanding that this verification is being called within a first-time query, we are guarenteed\n",
    "    to be searching by the name, so we just need to return a table with the other information\n",
    "    asked to verify. The name of this function is shorthand for \"first-time verify\".\n",
    "    \n",
    "    The logic can be changed to determine which information is being verified.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    soup -- the BeautifulSoup object containing the article we are verifying\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    \"\"\"\n",
    "    firstname = name.split()[0]\n",
    "    lastname = name.split()[1]\n",
    "    vtable = [False] * 2 # One table entry for every artifact we are trying to verify (name already verified)\n",
    "    \n",
    "    authors = soup.find_all('author')\n",
    "    for author in authors:\n",
    "        fname = author.find('forename')\n",
    "        lname = author.find('lastname')\n",
    "        \n",
    "        # We initially found this article with the name, so the author should always come up\n",
    "        if(fname and fname.text == firstname and lname and lname.text == lastname):\n",
    "            affils = author.find_all('affiliation')\n",
    "            for affil in affils:\n",
    "                if email and email in affil.text:\n",
    "                    vtable[0] = True\n",
    "                if org and org in affil.text:\n",
    "                    vtable[1] = True\n",
    "    \n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_tags(tags, reduce_num=10):\n",
    "    \"\"\"\n",
    "    Reduces a list of tags to a specified number based on some criteria.\n",
    "    \n",
    "    We want tags to \"look nice\" when presented to the user, so we can attempt to filter out\n",
    "    tags based off of three criteria. In order, we filter out numbers, dashes, spaces, and what\n",
    "    should be left is a single word without numbers.\n",
    "    \n",
    "    The logic can be changed to determine which criteria is checked for.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    tags -- the list of tags to be sorted through\n",
    "    \"\"\"\n",
    "    ideal_tags = []\n",
    "    mword_tags = []\n",
    "    dash_tags = []\n",
    "    num_tags = []\n",
    "    \n",
    "    for tag in tags:\n",
    "        if bool(re.search(r'\\d', tag)):\n",
    "            num_tags.append(tag)\n",
    "        elif bool(re.search(r'-', tag)):\n",
    "            dash_tags.append(tag)\n",
    "        elif bool(re.search(r\"\\s\", tag)):\n",
    "            mword_tags.append(tag)\n",
    "        else:\n",
    "            ideal_tags.append(tag)\n",
    "    \n",
    "    topics = ideal_tags + mword_tags + dash_tags + num_tags\n",
    "    \n",
    "    return topics[:reduce_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article_and_tags(soup):\n",
    "    \"\"\"\n",
    "    Generates an article dictionary and keywords gained from that article\n",
    "    \n",
    "    This method is broken into two sections: Retrieving article information and tag information.\n",
    "    It is highly reccommended to include a field for the DOI name and/or the PMID\n",
    "    as it would make future queries much easier.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    soup -- the BeautifulSoup object containing the article\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Article Section #\n",
    "    ###################\n",
    "    \n",
    "    article = {'title': 'None', 'publication date': 'None', 'pdf link': 'None', 'description': 'None'}\n",
    "    \n",
    "    # Article Title\n",
    "    title = soup.find('articletitle')\n",
    "    if title:\n",
    "        article['title'] = title.text\n",
    "    \n",
    "    # Publication Date\n",
    "    # Sometimes only part of the date has been provided\n",
    "    date = 'None'\n",
    "    pubdate = soup.find('pubdate')\n",
    "    if pubdate:\n",
    "        date = pubdate.year.text\n",
    "        if pubdate.month:\n",
    "            month = month_to_num(pubdate.month.text)       \n",
    "            date += '/' + month\n",
    "            if pubdate.day:\n",
    "                date += '/' + pubdate.day.text\n",
    "    article['publication date'] = date\n",
    "    \n",
    "    # Pdf Link\n",
    "    # All published articles should have a DOI name\n",
    "    doi = soup.find('articleid', idtype = \"doi\")\n",
    "    if doi:\n",
    "        # The pdf link should be the final redirect of accessing DOI name\n",
    "        pdf_link = urllib.request.urlopen('https://doi.org/' + doi.text)\n",
    "        article['pdf link'] = pdf_link.geturl()\n",
    "    \n",
    "    # Abstract\n",
    "    abst = soup.find('abstracttext')\n",
    "    if abst:\n",
    "        summary = NLP(abst.text)\n",
    "        article['description'] = summary\n",
    "    \n",
    "    ################\n",
    "    # Tags Section #\n",
    "    ################\n",
    "    \n",
    "    # Temporarily add all of the tags found to a list. Manipulation of the list comes after\n",
    "    # all tags we are taking have been collected (in generate_researcher).\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    keywords = soup.find_all('keyword')\n",
    "    for keyword in keywords:\n",
    "        tags.append(keyword.text)\n",
    "    \n",
    "    meshheadings = soup.find_all('meshheading')\n",
    "    for meshheading in meshheadings:\n",
    "        tags.append(meshheading.find('descriptorname').text)\n",
    "    \n",
    "    return article, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_num(month):\n",
    "    \"\"\"\n",
    "    Converts PubMed's month names and abbreviations to a numerical format\n",
    "    \n",
    "    This method should leave months alone if they are already a number.\n",
    "    If a new spelling is found, please add it to the hardcoded list.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    month -- the month name or number\n",
    "    \"\"\"\n",
    "    if month == 'Jan' or month == 'January':\n",
    "        month = '01'\n",
    "    elif month == 'Feb' or month == 'February':\n",
    "        month = '02'\n",
    "    elif month == 'Mar' or month == 'March':\n",
    "        month = '03'\n",
    "    elif month == 'Apr' or month == 'April':\n",
    "        month = '04'\n",
    "    elif month == 'May':\n",
    "        month = '05'\n",
    "    elif month == 'Jun' or month == 'June':\n",
    "        month = '06'\n",
    "    elif month == 'Jul' or month == 'July':\n",
    "        month = '07'\n",
    "    elif month == 'Aug' or month == 'August':\n",
    "        month = '08'\n",
    "    elif month == 'Sep' or month == 'Sept' or month == 'September':\n",
    "        month = '09'\n",
    "    elif month == 'Oct' or month == 'October':\n",
    "        month = '10'\n",
    "    elif month == 'Nov' or month == 'November':\n",
    "        month = '11'\n",
    "    elif month == 'Dec' or month == 'December':\n",
    "        month = '12'\n",
    "    \n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(description):\n",
    "    \"\"\"\n",
    "    Applies NLP on a description to produce a summmary.\n",
    "    \n",
    "    Specifically breaks a description down into sentences, then words. It counts each\n",
    "    word and assigns a value to each word based on its relative frequency within the passage.\n",
    "    Then it assigns each sentence a score based on its word values, finally taking\n",
    "    the best few sentences in the order of their scores.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    description -- the description to reduce\n",
    "    \"\"\"\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    formatted_desc = re.sub('[^a-zA-z]', ' ', description)\n",
    "    formatted_desc = re.sub(r'\\s+', ' ', formatted_desc)\n",
    "    formatted_desc = formatted_desc.lower()\n",
    "    sentence_list = nltk.sent_tokenize(description)\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_desc):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    maximum_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Andrew Garcia', 'email': 'None', 'organization': 'None', 'topics': ['adenoidectomy', 'anesthesia', 'apnea', 'midazolam', 'tonsillectomy', 'Humans', 'Kinetics', 'Ligands', 'Neuromodulation', 'Humans'], 'pmid': '33728122', 'studies': [{'title': 'Resistance to Pyrrolobenzodiazepine Dimers Is Associated with SLFN11 Downregulation and Can Be Reversed through Inhibition of ATR.', 'publication date': '2021/03', 'pdf link': 'https://mct.aacrjournals.org/content/20/3/541', 'description': 'We established a PBD-resistant cell line, 361-PBDr, by treating human breast cancer MDA-MB-361 cells with gradually increasing concentrations of SG3199, the PBD dimer released from the PBD drug-linker tesirine. 361-PBDr cells were over 20-fold less sensitive to SG3199 compared with parental cells and were cross-resistant to other PBD warhead and ADCs conjugated with PBDs. Proteomic profiling revealed that downregulation of Schlafen family member 11 (SLFN11), a putative DNA/RNA helicase, sensitizing cancer cells to DNA-damaging agents, was associated with PBD resistance. Collectively, these data provide insights into potential development of resistance to PBDs and PBD-conjugated ADCs, and more importantly, inform strategy development to overcome such resistance. Confirmatory studies demonstrated that siRNA knockdown of SLFN11 in multiple tumor cell lines conferred reduced sensitivity to SG3199 and PBD-conjugated ADCs. Resistance to antibody-drug conjugates (ADCs) has been observed in both preclinical models and clinical studies.'}, {'title': 'Effects of Premedication With Midazolam on Recovery and Discharge Times After Tonsillectomy and Adenoidectomy.', 'publication date': '2021/02/03', 'pdf link': 'https://www.cureus.com/articles/43524-effects-of-premedication-with-midazolam-on-recovery-and-discharge-times-after-tonsillectomy-and-adenoidectomy', 'description': 'Results Emergence and discharge times were 5.2 minutes (95% CI [-7.1, 17.4]; p=0.41) and 10.1 minutes (95% CI [-6.7, 26.8]; p=0.24) longer in MG. Conclusions Premedication with midazolam was not associated with prolonged emergence or discharge time or higher incidence of complications after anesthesia for T&A in patients with OSA. Midazolam receiving patients (midazolam group: MG) were compared to patients who did not (non-midazolam group: NMG). Adverse effects data in pediatric patients with obstructive sleep apnea (OSA) undergoing tonsillectomy and adenoidectomy (T&A) is limited. Aims We hypothesized that preoperative midazolam increases the time to emergence from anesthesia and postoperative discharge. Secondary objectives assessed if patients receiving midazolam experienced increased side effects or complications from treatment. The incidence of postoperative complications was comparable between MG and NMG groups.'}, {'title': 'Long-acting antibody ligand mimetics for HER4-selective agonism.', 'publication date': '2020/10/14', 'pdf link': 'https://www.nature.com/articles/s41598-020-74176-9?error=cookies_not_supported&code=9010e639-bba9-4ff6-9b62-f532aac02ee4', 'description': 'We developed an agonistic antibody modality, termed antibody ligand mimetics (ALM), by incorporating complex ligand agonists such as NRG1 into an antibody scaffold. Neuregulin protein 1 (NRG1) is a large (>â€‰60-amino-acid) natural peptide ligand for the ErbB protein family members HER3 and HER4.'}, {'title': 'A Comprehensive Review of the Diagnosis, Treatment, and Management of Postmastectomy Pain Syndrome.', 'publication date': '2020/06/11', 'pdf link': 'https://link.springer.com/article/10.1007/s11916-020-00876-6?error=cookies_not_supported&code=60a52904-3a2b-48fb-a0bc-0b205463e834', 'description': 'Postmastectomy pain syndrome (PMPS) remains poorly defined, although it is applied to chronic neuropathic pain following surgical procedures of the breast, including mastectomy and lumpectomy in breast-conserving surgery. Though the onset of pain is most likely to occur after surgery, there may also be a new onset of symptoms following adjuvant therapy, including chemotherapy or radiation therapy. It is characterized by persistent pain affecting the anterior thorax, axilla, and/or medial upper arm following mastectomy or lumpectomy.'}, {'title': 'A Comprehensive Review of the Diagnosis, Treatment, and Management of Urologic Chronic Pelvic Pain Syndrome.', 'publication date': '2020/05/06', 'pdf link': 'https://link.springer.com/article/10.1007/s11916-020-00857-9?error=cookies_not_supported&code=9792bfae-f6f9-4047-85ac-943fb0704407', 'description': 'Urologic chronic pelvic pain syndrome (UCPPS) is a chronic, noncyclic pain condition which can lead to significant patient morbidity and disability. It is defined by pain in the pelvic region, lasting for greater than 3 to 6Â\\xa0months, with no readily identifiable disease process. The aim of this review is to provide a comprehensive update of diagnosis and treatment of UCPPS.'}]}\n"
     ]
    }
   ],
   "source": [
    "print(generate_researcher('Andrew Garcia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
