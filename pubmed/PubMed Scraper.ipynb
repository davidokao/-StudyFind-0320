{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_researcher(name, email=None, org=None, studyNum=5, doi=None, pmid=None):\n",
    "    \"\"\"\n",
    "    Generates a researcher dictionary given a name and other identifying information\n",
    "    \n",
    "    This method is broken into two sections: Link generation and researcher generation.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default 'None')\n",
    "    org -- the organization of the researcher (default 'None')\n",
    "    studyNum -- the requested number of studies returned (default 5)\n",
    "    doi -- the DOI name of an existing publication by this researcher (default None)\n",
    "    pmid -- the PubMed ID of an existing article by this researcher (default None)\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Link Generation #\n",
    "    ###################\n",
    "    \n",
    "    # Search Link Creation\n",
    "    if not pmid and not doi:\n",
    "        print(\"going to fsttq\")\n",
    "        pmid = firsttime_query(name, email, org)\n",
    "    elif not pmid:\n",
    "        print(\"going to doi\")\n",
    "        pmid = doi_to_pmid(doi)\n",
    "    \n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    \n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    url = base + \"&retmax=\" + str(studyNum) + \"&term=\" + query\n",
    "    \n",
    "    if pmid:\n",
    "        url += \"&cauthor_id=\" + pmid\n",
    "    \n",
    "    print(\"pmid:\")\n",
    "    print(pmid)\n",
    "    return None\n",
    "    \n",
    "    # Create List of Articles\n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    #########################\n",
    "    # Researcher Generation #\n",
    "    #########################\n",
    "    \n",
    "    researcher = {'name': name, 'email': 'None', 'organization': 'None', 'topics': 'None', 'studies': 'None'}\n",
    "    articles = []\n",
    "    topics = []\n",
    "    apiRequestCounter = 0\n",
    "    \n",
    "    for PubMedID in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "        url = url.replace('idlist', PubMedID)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        article, tags = generate_article_and_tags(soup)\n",
    "        \n",
    "        # Occationally I ran into blank studies, this will temporarily remove them\n",
    "        # until I can find out what is causing this\n",
    "        ADD = False\n",
    "        for entries in study.values():\n",
    "            if entries != 'None':\n",
    "                ADD = True\n",
    "        if ADD:\n",
    "            articles.append(article)\n",
    "            topics = topics + tags\n",
    "        else:\n",
    "            print('Blank study encountered. Please save query information for further replication.')\n",
    "        \n",
    "        apiRequestCounter += 1\n",
    "        if apiRequestCounter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep in between.\n",
    "            apiRequestCounter = 0\n",
    "    \n",
    "    if email:\n",
    "        researcher['email'] = email\n",
    "    if org:\n",
    "        researcher['organization'] = org\n",
    "    if articles:\n",
    "        articles = sorted(articles, key = lambda i: i['publication date'], reverse=True)\n",
    "        researcher['studies'] = articles\n",
    "    if topics:\n",
    "        researcher['topics'] = topics\n",
    "    \n",
    "    return researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_to_pmid(doi):\n",
    "    \"\"\"\n",
    "    Returns a valid PubMedID if it exists.\n",
    "    \n",
    "    Given a valid DOI name or the None object, use the NCBI id converter API to potentially produce a PubMed ID.\n",
    "    If no results are obtained, the None object should be returned.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    doi -- the DOI name of the publication to be searched\n",
    "    \"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    \n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids=\" + doi + \"&format=json\"\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    pmid = dict_page[\"records\"][0][\"pmid\"]\n",
    "    \n",
    "    return pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firsttime_query(name, email=None, org=None):\n",
    "    \"\"\"\n",
    "    Searches for verifiable articles for a particular researcher\n",
    "    \n",
    "    If we have not previously searched for this researcher, this method should run a search of the first\n",
    "    X articles and pick one that\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    \n",
    "    Returns: PMID\n",
    "    \"\"\"\n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    url = base + \"&retmax=10&term=\" + query\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    apiRequestCounter = 0\n",
    "    for PubMedID in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "        url = url.replace('idlist', PubMedID)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        print(\"yoho\")\n",
    "        if verify(soup):\n",
    "            return PubMedID\n",
    "        \n",
    "        apiRequestCounter += 1\n",
    "        if apiRequestCounter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep in between.\n",
    "            apiRequestCounter = 0\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(soup, name=None, email=None, org=None, keywords=None):\n",
    "    \"\"\"\n",
    "    Verifies information in a given PubMed article\n",
    "    \n",
    "    TODO:\n",
    "    Return True or False? We might want a list of TFs for each identifying info given and found\n",
    "    \n",
    "    Keyword arguments:\n",
    "    pmid -- the PubMed ID of the article we are verifying\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    keywords -- the topics the researcher works with\n",
    "    \"\"\"\n",
    "    truthtable = [False] * 5\n",
    "    \n",
    "    authors = soup.find_all('author')\n",
    "    for author in authors:\n",
    "        fname = author.find('firstname')\n",
    "        lname = author.find('lastname')\n",
    "        affil = author.find_all('affiliationinfo')\n",
    "        print(author)\n",
    "        if(fname and fname == name.split()[0]):\n",
    "            truthtable[0] = True\n",
    "        if(lname and lname == name.split()[1]):\n",
    "            truthtable[1] = True\n",
    "        print(affil)\n",
    "        for affiliation in author['affiliationinfo']:\n",
    "            print(affiliation)\n",
    "    print(truthtable)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article_and_tags(soup):\n",
    "    \"\"\"\n",
    "    Generates an article dictionary and keywords gained from that article\n",
    "    \n",
    "    This method is broken into two sections: Retrieving article information and tag information.\n",
    "    It is highly reccommended to include a field for the DOI name and/or the PMID\n",
    "    as it would make future queries much easier.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    soup -- the BeautifulSoup object containing an article\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Article Section #\n",
    "    ###################\n",
    "    \n",
    "    article = {'title': 'None', 'publication date': 'None', 'pdf link': 'None', 'description': 'None'}\n",
    "    \n",
    "    # Article Title\n",
    "    title = soup.find('articletitle')\n",
    "    if title:\n",
    "        article['title'] = title.text\n",
    "    \n",
    "    # Publication Date\n",
    "    # Sometimes only part of the date has been provided\n",
    "    pubdate = soup.find('pubdate')\n",
    "    date = 'None'\n",
    "    if pubdate:\n",
    "        date = pubdate.year.text\n",
    "        if pubdate.month:\n",
    "            month = monthToNum(pubdate.month.text)       \n",
    "            date += '/' + pubdate.month.text\n",
    "            if pubdate.day:\n",
    "                date += '/' + pubdate.day.text\n",
    "    article['publication date'] = date\n",
    "    \n",
    "    # Pdf Link\n",
    "    # All published articles should have a DOI name\n",
    "    doi = soup.find('articleid', idtype = \"doi\")\n",
    "    if doi:\n",
    "        # The pdf link should be the final redirect of accessing DOI name\n",
    "        pdflink = urllib.request.urlopen('https://doi.org/' + doi.text)\n",
    "        article['pdf link'] = pdflink.geturl()\n",
    "    \n",
    "    # Abstract\n",
    "    abst = soup.find('abstracttext')\n",
    "    if desc:\n",
    "        article['description'] = abst.text\n",
    "    \n",
    "    ################\n",
    "    # Tags Section #\n",
    "    ################\n",
    "    \n",
    "    # Temporary implementation of tag collection\n",
    "    tags = []\n",
    "    keywords = soup.find_all('keyword')\n",
    "    for keyword in keywords:\n",
    "        tags.append(keyword.text)\n",
    "    # Could add MeSH tags if exist, would probably be better than these\n",
    "    \n",
    "    return study, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthToNum(month):\n",
    "    \"\"\"\n",
    "    Converts PubMed's month names and abbreviations to a numerical format\n",
    "    \n",
    "    This method should leave months alone if they are already a number.\n",
    "    If a new spelling is found, please add it to the hardcoded list.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    month -- the month name or number\n",
    "    \"\"\"\n",
    "    if month == 'Jan' or month == 'January':\n",
    "        month = '01'\n",
    "    elif month == 'Feb' or month == 'February':\n",
    "        month = '02'\n",
    "    elif month == 'Mar' or month == 'March':\n",
    "        month = '03'\n",
    "    elif month == 'Apr' or month == 'April':\n",
    "        month = '04'\n",
    "    elif month == 'May':\n",
    "        month = '05'\n",
    "    elif month == 'Jun' or month == 'June':\n",
    "        month = '06'\n",
    "    elif month == 'Jul' or month == 'July':\n",
    "        month = '07'\n",
    "    elif month == 'Aug' or month == 'August':\n",
    "        month = '08'\n",
    "    elif month == 'Sep' or month == 'Sept' or month == 'September':\n",
    "        month = '09'\n",
    "    elif month == 'Oct' or month == 'October':\n",
    "        month = '10'\n",
    "    elif month == 'Nov' or month == 'November':\n",
    "        month = '11'\n",
    "    elif month == 'Dec' or month == 'December':\n",
    "        month = '12'\n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(description):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    for stoo in profile['studies']:\n",
    "        formatted_desc = re.sub('[^a-zA-z]', ' ', stoo['description'])\n",
    "        formatted_desc = re.sub(r'\\s+', ' ', formatted_desc)\n",
    "        formatted_desc = formatted_desc.lower()\n",
    "        sentence_list = nltk.sent_tokenize(stoo['description'])\n",
    "        \n",
    "        word_frequencies = {}\n",
    "        for word in nltk.word_tokenize(formatted_desc):\n",
    "            if word not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "        \n",
    "        maximum_frequency = max(word_frequencies.values())\n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "        \n",
    "        sentence_scores = {}\n",
    "        for sent in sentence.list:\n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            sentence_scores[sent] = word_frequencies[word]\n",
    "                        else:\n",
    "                            sentence_scores[sent] += word_frequencies[word]\n",
    "        \n",
    "        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        print(stoo['description'])\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to fsttq\n",
      "yoho\n",
      "[False, False, False, False, False]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-5a6281916af9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_researcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Andrew Garcia'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d0c062d549a9>\u001b[0m in \u001b[0;36mgenerate_researcher\u001b[1;34m(name, email, org, studyNum, doi, pmid)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&retmax=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstudyNum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"&term=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpmid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "print(generate_researcher('Andrew Garcia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
