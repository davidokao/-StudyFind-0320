{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# One of the pdf links I got was\n",
    "# https://link.springer.com/article/10.1007/s11916-020-00876-6?\n",
    "# error=cookies_not_supported&code=aaf75d42-e149-43f5-b8bb-79939e10f065\n",
    "# indica\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_researcher(name, email=None, org=None, studyNum=5, doi=None, pmid=None):\n",
    "    \"\"\"\n",
    "    Generates a researcher dictionary given a name and other identifying information\n",
    "    \n",
    "    This method is broken into two sections: Link generation and researcher generation.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default 'None')\n",
    "    org -- the organization of the researcher (default 'None')\n",
    "    studyNum -- the requested number of studies returned (default 5)\n",
    "    doi -- the DOI name of an existing publication by this researcher (default None)\n",
    "    pmid -- the PubMed ID of an existing article by this researcher (default None)\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Link Generation #\n",
    "    ###################\n",
    "    \n",
    "    # Search Link Creation\n",
    "    if doi and not pmid:\n",
    "        pmid = doi_to_pmid(doi)\n",
    "    elif not pmid:\n",
    "        pmid = ft_query(name, email, org)\n",
    "        time.sleep(0.1) # UrlLib requires sleep time between queries, which occurs in ft_query and below\n",
    "    \n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    \n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    url = base + \"&retmax=\" + str(studyNum) + \"&term=\" + query + \"&cauthor_id=\" + pmid\n",
    "    \n",
    "    # Create List of Articles\n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    #########################\n",
    "    # Researcher Generation #\n",
    "    #########################\n",
    "    \n",
    "    researcher = {'name': name, 'email': 'None', 'organization': 'None', 'topics': 'None', 'studies': 'None'}\n",
    "    articles = []\n",
    "    topics = []\n",
    "    apiRequestCounter = 0\n",
    "    \n",
    "    for PubMedID in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "        url = url.replace('idlist', PubMedID)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        article, tags = generate_article_and_tags(soup)\n",
    "        \n",
    "        # Occationally I ran into blank studies, this will temporarily remove them\n",
    "        # until I can find out what is causing this\n",
    "        ADD = False\n",
    "        for entries in article.values():\n",
    "            if entries != 'None':\n",
    "                ADD = True\n",
    "        if ADD:\n",
    "            articles.append(article)\n",
    "            topics = topics + tags\n",
    "        else:\n",
    "            print('Blank study encountered. Please save query information for further replication.')\n",
    "        \n",
    "        apiRequestCounter += 1\n",
    "        if apiRequestCounter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep in between.\n",
    "            apiRequestCounter = 0\n",
    "    \n",
    "    if email:\n",
    "        researcher['email'] = email\n",
    "    if org:\n",
    "        researcher['organization'] = org\n",
    "    if articles:\n",
    "        articles = sorted(articles, key = lambda i: i['publication date'], reverse=True)\n",
    "        researcher['studies'] = articles\n",
    "    if topics:\n",
    "        researcher['topics'] = topics\n",
    "    if doi:\n",
    "        researcher['doi'] = doi\n",
    "    if pmid:\n",
    "        researcher['pmid'] = pmid # There should ALWAYS be a pmid returned.\n",
    "    else:\n",
    "        raise Exception(\"No PMID returned, something has gone horribly wrong\")\n",
    "    return researcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doi_to_pmid(doi):\n",
    "    \"\"\"\n",
    "    Returns a valid PubMedID if it exists.\n",
    "    \n",
    "    Given a valid DOI name or the None object, use the NCBI id converter API to potentially produce a PubMed ID.\n",
    "    If no results are obtained, the None object should be returned.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    doi -- the DOI name of the publication to be searched\n",
    "    \"\"\"\n",
    "    if not doi:\n",
    "        return None\n",
    "    \n",
    "    url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids=\" + doi + \"&format=json\"\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    pmid = dict_page[\"records\"][0][\"pmid\"]\n",
    "    \n",
    "    return pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_query(name, email=None, org=None):\n",
    "    \"\"\"\n",
    "    Searches for verifiable articles for a particular researcher, with the prior information\n",
    "    that this is the first-time query being performed.\n",
    "    \n",
    "    If we have not previously searched for this researcher, this method should run a search of the first\n",
    "    10 articles and pick the one with the most verifiable information.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    \"\"\"\n",
    "    query = name.split()[1] + \"+\" + name.split()[0]\n",
    "    base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&retmode=json\"\n",
    "    url = base + \"&retmax=10&term=\" + query\n",
    "    \n",
    "    webpage = urllib.request.urlopen(url).read()\n",
    "    dict_page =json.loads(webpage)\n",
    "    idlist = dict_page[\"esearchresult\"][\"idlist\"]\n",
    "    \n",
    "    apiRequestCounter = 0\n",
    "    PubMedIDList = []\n",
    "    pmid = None\n",
    "    for PubMedID in idlist:\n",
    "        url = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&retmode=xml&id=idlist\"\n",
    "        url = url.replace('idlist', PubMedID)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        \n",
    "        vtable = ft_verify(soup, name, email, org)\n",
    "        if vtable[0]:\n",
    "            PubMedIDList.insert(0, PubMedID)\n",
    "        elif vtable[1]:\n",
    "            PubMedIDList.append(PubMedID)\n",
    "        elif not pmid:\n",
    "            pmid = PubMedID\n",
    "        \n",
    "        apiRequestCounter += 1\n",
    "        if apiRequestCounter == 3:\n",
    "            time.sleep(0.2) # API allows for 3 queries at a time. Need to sleep in between.\n",
    "            apiRequestCounter = 0\n",
    "    \n",
    "    if len(PubMedIDList) > 0:\n",
    "        pmid = PubMedIDList[0]\n",
    "    return pmid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_verify(soup, name, email=None, org=None):\n",
    "    \"\"\"\n",
    "    Verifies information in a given PubMed article, with the prior information\n",
    "    that this is within the first-time query being performed.\n",
    "    \n",
    "    Understanding this verification is being called within a first-time query, we are guarenteed\n",
    "    to be searching by the name, so we just need to return a table with the other information\n",
    "    asked to verify.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    pmid -- the PubMed ID of the article we are verifying\n",
    "    name -- the name of the researcher\n",
    "    email -- the email of the researcher (default None)\n",
    "    org -- the organization of the researcher (default None)\n",
    "    \"\"\"\n",
    "    firstname = name.split()[0]\n",
    "    lastname = name.split()[1]\n",
    "    vtable = [False] * 2\n",
    "    \n",
    "    authors = soup.find_all('author')\n",
    "    for author in authors:\n",
    "        fname = author.find('forename')\n",
    "        lname = author.find('lastname')\n",
    "        \n",
    "        # We initially found this article with the name, so the author should always come up\n",
    "        if(fname and fname.text == firstname and lname and lname.text == lastname):\n",
    "            affils = author.find_all('affiliation')\n",
    "            for affil in affils:\n",
    "                if email and email in affil.text:\n",
    "                    vtable[0] = True\n",
    "                if org and org in affil.text:\n",
    "                    vtable[1] = True\n",
    "    return vtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article_and_tags(soup):\n",
    "    \"\"\"\n",
    "    Generates an article dictionary and keywords gained from that article\n",
    "    \n",
    "    This method is broken into two sections: Retrieving article information and tag information.\n",
    "    It is highly reccommended to include a field for the DOI name and/or the PMID\n",
    "    as it would make future queries much easier.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    soup -- the BeautifulSoup object containing an article\n",
    "    \"\"\"\n",
    "    ###################\n",
    "    # Article Section #\n",
    "    ###################\n",
    "    \n",
    "    article = {'title': 'None', 'publication date': 'None', 'pdf link': 'None', 'description': 'None'}\n",
    "    \n",
    "    # Article Title\n",
    "    title = soup.find('articletitle')\n",
    "    if title:\n",
    "        article['title'] = title.text\n",
    "    \n",
    "    # Publication Date\n",
    "    # Sometimes only part of the date has been provided\n",
    "    pubdate = soup.find('pubdate')\n",
    "    date = 'None'\n",
    "    if pubdate:\n",
    "        date = pubdate.year.text\n",
    "        if pubdate.month:\n",
    "            month = monthToNum(pubdate.month.text)       \n",
    "            date += '/' + pubdate.month.text\n",
    "            if pubdate.day:\n",
    "                date += '/' + pubdate.day.text\n",
    "    article['publication date'] = date\n",
    "    \n",
    "    # Pdf Link\n",
    "    # All published articles should have a DOI name\n",
    "    doi = soup.find('articleid', idtype = \"doi\")\n",
    "    if doi:\n",
    "        # The pdf link should be the final redirect of accessing DOI name\n",
    "        pdflink = urllib.request.urlopen('https://doi.org/' + doi.text)\n",
    "        article['pdf link'] = pdflink.geturl()\n",
    "    \n",
    "    # Abstract\n",
    "    abst = soup.find('abstracttext')\n",
    "    if abst:\n",
    "        article['description'] = abst.text\n",
    "    \n",
    "    ################\n",
    "    # Tags Section #\n",
    "    ################\n",
    "    \n",
    "    # Temporary implementation of tag collection\n",
    "    tags = []\n",
    "    keywords = soup.find_all('keyword')\n",
    "    for keyword in keywords:\n",
    "        tags.append(keyword.text)\n",
    "    # Could add MeSH tags if exist, would probably be better than these\n",
    "    \n",
    "    return article, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthToNum(month):\n",
    "    \"\"\"\n",
    "    Converts PubMed's month names and abbreviations to a numerical format\n",
    "    \n",
    "    This method should leave months alone if they are already a number.\n",
    "    If a new spelling is found, please add it to the hardcoded list.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    month -- the month name or number\n",
    "    \"\"\"\n",
    "    if month == 'Jan' or month == 'January':\n",
    "        month = '01'\n",
    "    elif month == 'Feb' or month == 'February':\n",
    "        month = '02'\n",
    "    elif month == 'Mar' or month == 'March':\n",
    "        month = '03'\n",
    "    elif month == 'Apr' or month == 'April':\n",
    "        month = '04'\n",
    "    elif month == 'May':\n",
    "        month = '05'\n",
    "    elif month == 'Jun' or month == 'June':\n",
    "        month = '06'\n",
    "    elif month == 'Jul' or month == 'July':\n",
    "        month = '07'\n",
    "    elif month == 'Aug' or month == 'August':\n",
    "        month = '08'\n",
    "    elif month == 'Sep' or month == 'Sept' or month == 'September':\n",
    "        month = '09'\n",
    "    elif month == 'Oct' or month == 'October':\n",
    "        month = '10'\n",
    "    elif month == 'Nov' or month == 'November':\n",
    "        month = '11'\n",
    "    elif month == 'Dec' or month == 'December':\n",
    "        month = '12'\n",
    "    return month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP(description):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    for stoo in profile['studies']:\n",
    "        formatted_desc = re.sub('[^a-zA-z]', ' ', stoo['description'])\n",
    "        formatted_desc = re.sub(r'\\s+', ' ', formatted_desc)\n",
    "        formatted_desc = formatted_desc.lower()\n",
    "        sentence_list = nltk.sent_tokenize(stoo['description'])\n",
    "        \n",
    "        word_frequencies = {}\n",
    "        for word in nltk.word_tokenize(formatted_desc):\n",
    "            if word not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "        \n",
    "        maximum_frequency = max(word_frequencies.values())\n",
    "        for word in word_frequencies.keys():\n",
    "            word_frequencies[word] = (word_frequencies[word]/maximum_frequency)\n",
    "        \n",
    "        sentence_scores = {}\n",
    "        for sent in sentence.list:\n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            sentence_scores[sent] = word_frequencies[word]\n",
    "                        else:\n",
    "                            sentence_scores[sent] += word_frequencies[word]\n",
    "        \n",
    "        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "        summary = ' '.join(summary_sentences)\n",
    "        \n",
    "        print(stoo['description'])\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Andrew Garcia', 'email': 'None', 'organization': 'None', 'topics': ['adenoidectomy', 'anesthesia', 'apnea', 'midazolam', 'tonsillectomy', 'Breast surgery', 'Neuromodulation', 'Postmastectomy pain syndrome', 'Postsurgical pain', 'Chronic prostatitis', 'Interstitial cystitis', 'Painful bladder syndrome', 'Urologic chronic pelvic pain syndrome'], 'studies': [{'title': 'Effects of Premedication With Midazolam on Recovery and Discharge Times After Tonsillectomy and Adenoidectomy.', 'publication date': '2021/Feb/03', 'pdf link': 'https://www.cureus.com/articles/43524-effects-of-premedication-with-midazolam-on-recovery-and-discharge-times-after-tonsillectomy-and-adenoidectomy', 'description': 'Background Midazolam is commonly used preoperatively for anxiety. Adverse effects data in pediatric patients with obstructive sleep apnea (OSA) undergoing tonsillectomy and adenoidectomy (T&A) is limited. Aims We hypothesized that preoperative midazolam increases the time to emergence from anesthesia and postoperative discharge. Secondary objectives assessed if patients receiving midazolam experienced increased side effects or complications from treatment. Methods This study was a retrospective chart review of patients undergoing T&A from July 2014 to December 2015. Midazolam receiving patients (midazolam group: MG) were compared to patients who did not (non-midazolam group: NMG). Multivariable analyses were performed and adjusted for predefined potential cofounder variables. Results Emergence and discharge times were 5.2 minutes (95% CI [-7.1, 17.4]; p=0.41) and 10.1 minutes (95% CI [-6.7, 26.8]; p=0.24) longer in MG. These results were not statistically significant. Comparing by OSA status, there was no statistical difference in emergence and discharge times between mild, moderate and severe OSA groups or between MG and NMG within each OSA group. Emergence and discharge times in moderate OSA was 6.1 minutes (95% CI [-17.6, 29.8]; p=0.61) and 18.8 minutes (95% CI [-16.4, 53.9]; p=0.29) longer than mild OSA, and in the severe OSA group, 2.6 minutes (95% CI [-19.9, 25.1]; p=0.82) shorter and 2.8 minutes (95% CI [-30.3, 35.9]; p=0.87) longer. The incidence of postoperative complications was comparable between MG and NMG groups. Conclusions Premedication with midazolam was not associated with prolonged emergence or discharge time or higher incidence of complications after anesthesia for T&A in patients with OSA.'}, {'title': 'Resistance to Pyrrolobenzodiazepine Dimers Is Associated with SLFN11 Downregulation and Can Be Reversed through Inhibition of ATR.', 'publication date': '2021/03', 'pdf link': 'https://mct.aacrjournals.org/content/20/3/541', 'description': 'Resistance to antibody-drug conjugates (ADCs) has been observed in both preclinical models and clinical studies. However, mechanisms of resistance to pyrrolobenzodiazepine (PBD)-conjugated ADCs have not been well characterized and thus, this study was designed to investigate development of resistance to PBD dimer warheads and PBD-conjugated ADCs. We established a PBD-resistant cell line, 361-PBDr, by treating human breast cancer MDA-MB-361 cells with gradually increasing concentrations of SG3199, the PBD dimer released from the PBD drug-linker tesirine. 361-PBDr cells were over 20-fold less sensitive to SG3199 compared with parental cells and were cross-resistant to other PBD warhead and ADCs conjugated with PBDs. Proteomic profiling revealed that downregulation of Schlafen family member 11 (SLFN11), a putative DNA/RNA helicase, sensitizing cancer cells to DNA-damaging agents, was associated with PBD resistance. Confirmatory studies demonstrated that siRNA knockdown of SLFN11 in multiple tumor cell lines conferred reduced sensitivity to SG3199 and PBD-conjugated ADCs. Treatment with EPZ011989, an EZH2 inhibitor, derepressed SLFN11 expression in 361-PBDr and other SLFN11-deficient tumor cells, and increased sensitivity to PBD and PBD-conjugated ADCs, indicating that the suppression of SLFN11 expression is associated with histone methylation as reported. Moreover, we demonstrated that combining an ataxia telangiectasia and Rad3-related protein (ATR) inhibitor, AZD6738, with SG3199 or PBD-based ADCs led to synergistic cytotoxicity in either resistant 361-PBDr cells or cells that SLFN11 was knocked down via siRNA. Collectively, these data provide insights into potential development of resistance to PBDs and PBD-conjugated ADCs, and more importantly, inform strategy development to overcome such resistance.'}, {'title': 'A Comprehensive Review of the Diagnosis, Treatment, and Management of Urologic Chronic Pelvic Pain Syndrome.', 'publication date': '2020/May/06', 'pdf link': 'https://link.springer.com/article/10.1007/s11916-020-00857-9?error=cookies_not_supported&code=2a9362d2-4a97-4b62-91d1-d9689f58abdf', 'description': 'Urologic chronic pelvic pain syndrome (UCPPS) is a chronic, noncyclic pain condition which can lead to significant patient morbidity and disability. It is defined by pain in the pelvic region, lasting for greater than 3 to 6Â\\xa0months, with no readily identifiable disease process. The aim of this review is to provide a comprehensive update of diagnosis and treatment of UCPPS.'}, {'title': 'A Comprehensive Review of the Diagnosis, Treatment, and Management of Postmastectomy Pain Syndrome.', 'publication date': '2020/Jun/11', 'pdf link': 'https://link.springer.com/article/10.1007/s11916-020-00876-6?error=cookies_not_supported&code=aaf75d42-e149-43f5-b8bb-79939e10f065', 'description': 'Postmastectomy pain syndrome (PMPS) remains poorly defined, although it is applied to chronic neuropathic pain following surgical procedures of the breast, including mastectomy and lumpectomy in breast-conserving surgery. It is characterized by persistent pain affecting the anterior thorax, axilla, and/or medial upper arm following mastectomy or lumpectomy. Though the onset of pain is most likely to occur after surgery, there may also be a new onset of symptoms following adjuvant therapy, including chemotherapy or radiation therapy.'}, {'title': 'Long-acting antibody ligand mimetics for HER4-selective agonism.', 'publication date': '2020/10/14', 'pdf link': 'https://www.nature.com/articles/s41598-020-74176-9?error=cookies_not_supported&code=8341c446-1a4d-4f96-9cf0-ef046ff96c98', 'description': 'Neuregulin protein 1 (NRG1) is a large (>â€‰60-amino-acid) natural peptide ligand for the ErbB protein family members HER3 and HER4. We developed an agonistic antibody modality, termed antibody ligand mimetics (ALM), by incorporating complex ligand agonists such as NRG1 into an antibody scaffold. We optimized the linker and ligand length to achieve native ligand activity in HEK293 cells and cardiomyocytes derived from induced pluripotent stem cells (iPSCs) and used a monomeric Fc-ligand fusion platform to steer the ligand specificity toward HER4-dominant agonism. With the help of selectivity engineering, these enhanced ALM molecules can provide an antibody scaffold with increased receptor specificity and the potential to greatly improve the pharmacokinetics, stability, and downstream developability profiles from the natural ligand approach. This ligand mimetic design and optimization approach can be expanded to apply to other cardiovascular disease targets and emerging therapeutic areas, providing differentiated drug molecules with increased specificity and extended half-life.'}], 'pmid': '33728122'}\n"
     ]
    }
   ],
   "source": [
    "print(generate_researcher('Andrew Garcia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
