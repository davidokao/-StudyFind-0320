{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: window was already closed\n  (Session info: chrome=89.0.4389.90)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3994acfb5035>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tbody'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: window was already closed\n  (Session info: chrome=89.0.4389.90)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://clinicaltrials.gov/ct2/results?cond=&term=&cntry=&state=&city=&dist='\n",
    "driver = webdriver.Chrome('//Users/shardulkothapalli/Desktop/SCHOOL/6.spring2021/cs3312/StudyFind-0320/chromedriver')\n",
    "driver.get(url)\n",
    "time.sleep(4)\n",
    "soup = BeautifulSoup(driver.page_source,'html')\n",
    "driver.quit()\n",
    "tables = soup.find_all('tbody')\n",
    "print(tables[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Not yet recruitingNew', 'Fighting Climate Change: Urban Greennes, Active Mobility and Health Co-benefits.', '/ct2/show/NCT04742179?draw=2&amp;rank=1', 'Respiratory and Allergic Symptoms', 'Other: Manteinance and care intervention on the green areas.'], ['2', 'Not yet recruitingNew', 'Impact of Lateral Gastro-jejunal Anastomosis on the Rate of Gastroparesis After Cephalic Duodenopancreatectomy', '/ct2/show/NCT04742166?draw=2&amp;rank=2', 'Surgical Technique', 'Procedure: Reconstruction', 'Institut Paoli CalmettesMarseille, FranceCHU de RennesRennes, France'], ['3', 'Not yet recruitingNew', 'A Study of MRG002 in the Treatment of Patients With HER2-low Locally Advanced or Metastatic Breast Cancer (BC)', '/ct2/show/NCT04742153?draw=2&amp;rank=3', 'Advanced or Metastatic Breast Cancer', 'Drug: MRG002', 'Fifth Medical Center of PLA General HospitalBeijing, Beijing, ChinaThe Fourth hospital of Hebei Medical UniversityShijiazhuang, Hebei, China'], ['4', 'Not yet recruitingNew', 'Magnesium Sulphate Injection in Treatment of Myofascial Trigger Points', '/ct2/show/NCT04742140?draw=2&amp;rank=4', 'Trigger Point Pain, Myofascial', 'Drug: Magnesium sulfateDrug: Saline'], ['5', 'Active, not recruitingNew', 'Failure of a Single Surgical Debridement in Septic Arthritis of the Native Hip', '/ct2/show/NCT04742127?draw=2&amp;rank=5', 'Septic Arthritis of the Native HipFailure of Initial Debridement', 'Diagnostic Test: Single surgical debridement', 'UZ LeuvenLeuven, Belgium'], ['6', 'Not yet recruitingNew', 'Effect of EPAP Device on Emphysema and Lung Bullae', '/ct2/show/NCT04742114?draw=2&amp;rank=6', 'EmphysemaBullous Disease Lung', 'Device: use the face mask with Expiratory Positive Airway Pressure(EPAP).Device: use the face mask without Expiratory Positive Airway Pressure(EPAP).'], ['7', 'Not yet recruitingNew', 'Phase I/II Trial of S65487 Plus Azacitidine in Acute Myeloid Leukemia', '/ct2/show/NCT04742101?draw=2&amp;rank=7', 'Acute Myeloid Leukemia', 'Drug: S65487 and azacitidine'], ['8', 'RecruitingNew', 'Assessment of the Safety and Performance of a Compression Ankle Support in the Prevention of Injuries During Sports Practice', '/ct2/show/NCT04742088?draw=2&amp;rank=8', 'Chronic PainChronic Instability of Ankle Joint', 'Device: ankleSOFT100', 'KOSS Paris 8Paris, France'], ['9', 'Not yet recruitingNew', 'Olaparib, Durvalumab and UV1 in Relapsed Ovarian Cancer', '/ct2/show/NCT04742075?draw=2&amp;rank=9', 'Ovarian Cancer', 'Drug: Olaparib + durvalumab + UV1', 'RigshospitaletKøbenhavn Ø, Sjaelland, Denmark'], ['10', 'CompletedNew', 'First in Human Clinical Trial of ApTOLL in Healthy Volunteers', '/ct2/show/NCT04742062?draw=2&amp;rank=10', 'Stroke', 'Drug: ApTOLLOther: Placebo', 'Clinical Pharmacology Department. Hospital Universitario de La PrincesaMadrid, Spain']]\n"
     ]
    }
   ],
   "source": [
    "table = tables[1]\n",
    "big_info = []\n",
    "for row in table.find_all(\"tr\"):\n",
    "    row_list = [] \n",
    "    for cell in row.find_all([\"th\",\"td\"]):\n",
    "        temp = cell.text\n",
    "        if (temp != ''):\n",
    "            row_list.append(temp)\n",
    "        if \"href\" in str(cell):\n",
    "            href_tags = cell.find_all(href=True)\n",
    "            a = str(href_tags[0]).split(\" \")[1]        \n",
    "            row_list.append(a[6:-1])        \n",
    "    big_info.append(row_list)\n",
    "    \n",
    "print(big_info)\n",
    "\n",
    "            \n",
    "# tab_data = [[cell.text for cell in row.find_all([\"th\",\"td\"])]\n",
    "#                         for row in table.find_all(\"tr\")]\n",
    "# print(tab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRIEF SUMMARY:  Prospective bi-centric randomized open-label study comparing lateral and terminolateral gastro-jejunal anastomoses in cephalic duodenopancreatectomies. \n",
      "\n",
      "DETAILLED SUMMARY:  Gastroparesis is one of the main complications occurring after cephalic duodenopancreatectomy, the incidence of which is estimated between 10 and 40% in the literature. Its occurrence leads to an alteration in post-operative quality of life (maintenance or resting of the nasogastric tube) and is the primary reason for lengthening the length of hospital stay and thus increasing the cost of treatment. In addition, it predisposes to the risk of inhalation pneumopathy, which increases the risk of post-operative death. Various technical surgical points have been suggested by retrospective studies to reduce its incidence (pyloric preservation, respect for the left gastric vein, ante-colic positioning of the Child's handle, making a Y-shaped handle) but without ever being validated in randomized prospective studies.\n",
      "Recently three retrospective studies have highlighted the interest of performing a lateral-lateral rather than a terminolateral gastro-jejunal anastomosis to reduce the rate of post-operative gastroparesis. \n",
      "\n",
      "STUDY ELIGIBILITY:  ['Ages Eligible for Study: \\xa0 ', '18 Years and older \\xa0 (Adult, Older Adult)', 'Sexes Eligible for Study: \\xa0 ', 'All', 'Accepts Healthy Volunteers: \\xa0 ', 'No'] \n",
      "\n",
      "STUDY CRITERIA:  ['\\nStudy Type  :', '\\r\\n        Interventional\\r\\n                \\xa0(Clinical Trial)\\r\\n                \\r\\n        \\r\\n      ', 'Estimated\\r\\nEnrollment  :', '166 participants', 'Allocation:', 'Randomized', 'Intervention Model:', 'Parallel Assignment', 'Masking:', ' None (Open Label)', 'Primary Purpose:', ' Other', 'Official Title:', 'Impact of Lateral Gastro-jejunal Anastomosis on the Rate of Gastroparesis After Cephalic Duodenopancreatectomy: A Prospective Randomized Study', '  Estimated Study Start Date  :', 'April 1, 2021', '  Estimated Primary Completion Date  :', 'July 1, 2023', '  Estimated Study Completion Date  :', 'July 1, 2023'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for study in big_info[1:]:\n",
    "    temp_dict = {}\n",
    "    url = 'https://clinicaltrials.gov' + study[3]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html')\n",
    "    #print(soup)\n",
    "    summ = soup.findAll(\"div\", {\"class\": \"ct-body3 tr-indent2\"})\n",
    "    temp_dict['brief summary'] = summ[0].text\n",
    "    temp_dict['detailled description'] = summ[1].text\n",
    "    summ = soup.findAll(\"div\", {\"class\": \"ct-body3 tr-indent2\"})\n",
    "\n",
    "    study_design = soup.findAll(\"table\", {\"class\": \"ct-layout_table tr-tableStyle tr-studyInfo\"})\n",
    "    tab1 = study_design[0]\n",
    "    tab1_det = []\n",
    "    for row in tab1.find_all(\"tr\"):\n",
    "        for cell in row.find_all([\"th\",\"td\"]):\n",
    "            temp = cell.text\n",
    "            if temp != '':\n",
    "                tab1_det.append(temp)\n",
    "    tab2 = study_design[1]\n",
    "    tab2_det = []\n",
    "    for row in tab2.find_all(\"tr\"):\n",
    "        for cell in row.find_all([\"th\",\"td\"]):\n",
    "            temp = cell.text\n",
    "            if temp != '':\n",
    "                tab2_det.append(temp)\n",
    "    \n",
    "    study_design = soup.findAll(\"div\", {\"class\": \"tr-indent2\"})\n",
    "    print('BRIEF SUMMARY: ', temp_dict['brief summary'], '\\n')\n",
    "    print('DETAILLED SUMMARY: ', temp_dict['detailled description'], '\\n')\n",
    "    print(\"STUDY ELIGIBILITY: \", tab2_det, '\\n')\n",
    "    print(\"STUDY CRITERIA: \", tab1_det, '\\n')\n",
    "    #print(len(study_design))\n",
    "    #for row in tab1.find_all(\"tr\")\n",
    "    #print(study_design)\n",
    "\n",
    "    break\n",
    "driver.quit()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('//Users/shardulkothapalli/Desktop/SCHOOL/6.spring2021/cs3312/StudyFind-0320/chromedriver')\n",
    "url = 'https://scholar.google.com/citations?user=mG4imMEAAAAJ&hl=en'\n",
    "url += '&view_op=list_works&sortby=pubdate'\n",
    "driver.get(url)\n",
    "soup = BeautifulSoup(driver.page_source,'html')\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html')\n",
    "\n",
    "\n",
    "prof = soup.findAll(\"div\", {\"id\": \"gsc_prf_i\"})\n",
    "tags = soup.findAll(\"a\", {\"class\": \"gsc_prf_inta gs_ibl\"})\n",
    "prof_info = []\n",
    "\n",
    "for row in prof[0].find_all(\"div\")[:2]:\n",
    "    prof_info.append(row.text)\n",
    "temp_tags = []\n",
    "for tag in tags:\n",
    "    temp_tags.append(tag.text)\n",
    "prof_info.append(temp_tags)\n",
    "prof_pic = driver.find_element_by_xpath('//*[@id=\"gsc_prf_pup-img\"]')\n",
    "prof_pic = prof_pic.get_attribute('src')\n",
    "\n",
    "studies = soup.findAll(\"tbody\", {\"id\": \"gsc_a_b\"})\n",
    "count = 1\n",
    "\n",
    "stoodies = []\n",
    "for row in studies[0].find_all(\"tr\", {'class': 'gsc_a_tr'}):\n",
    "    xp = '//*[@id=\"gsc_a_b\"]/tr[' + str(count) + ']/td[1]/a'\n",
    "    elem = driver.find_element_by_xpath(xp)\n",
    "    elem.click()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    try:\n",
    "        title = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_title\"]/a')\n",
    "        title = title.text\n",
    "    except:\n",
    "        title = 'no title'\n",
    "    \n",
    "    try:\n",
    "        pubdate = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_table\"]/div[2]/div[2]')\n",
    "        pubdate = pubdate.text\n",
    "    except:\n",
    "        pubdate = 'no date'\n",
    "        \n",
    "    try:\n",
    "        pdflink = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_title_gg\"]/div/a')\n",
    "        pdflink = pdflink.get_attribute('href')\n",
    "    except:\n",
    "        pdflink = 'no link'\n",
    "        \n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_descr\"]/div/div')\n",
    "        desc = desc.text\n",
    "    except:\n",
    "        desc = 'no description'\n",
    "    \n",
    "    study = {'title': title, 'publication date': pubdate, 'pdf link': pdflink, 'description': desc}\n",
    "    stoodies.append(study)\n",
    "    \n",
    "    elem = driver.find_element_by_xpath('//*[@id=\"gs_md_cita-d-x\"]/span[1]')\n",
    "    elem.click()\n",
    "    \n",
    "    count += 1    \n",
    "driver.quit()\n",
    "profile = {'name': prof_info[0], 'organization': prof_info[1], 'topics': prof_info[2], 'profile pic': prof_pic,\n",
    "           'studies': stoodies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Andrew Ng',\n",
       " 'organization': 'Stanford University',\n",
       " 'topics': ['Machine Learning', 'Deep Learning', 'AI'],\n",
       " 'profile pic': 'https://scholar.googleusercontent.com/citations?view_op=view_photo&user=mG4imMEAAAAJ&citpid=1',\n",
       " 'studies': [{'title': 'CheXbreak: Misclassification Identification for Deep Learning Models Interpreting Chest X-rays',\n",
       "   'publication date': '2021/3/18',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2103.09957',\n",
       "   'description': 'A major obstacle to the integration of deep learning models for chest x-ray interpretation into clinical settings is the lack of understanding of their failure modes. In this work, we first investigate whether there are patient subgroups that chest x-ray models are likely to misclassify. We find that patient age and the radiographic finding of lung lesion or pneumothorax are statistically relevant features for predicting misclassification for some chest x-ray models. Second, we develop misclassification predictors on chest x-ray models using their outputs and clinical features. We find that our best performing misclassification identifier achieves an AUROC close to 0.9 for most diseases. Third, employing our misclassification identifiers, we develop a corrective algorithm to selectively flip model predictions that have high likelihood of misclassification at inference time. We observe F1 improvement on the prediction of Consolidation (0.008 [95\\\\% CI 0.005, 0.010]) and Edema (0.003,[95\\\\% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and high-performing chest x-ray models, we are able to derive insights across model architectures and offer a generalizable framework applicable to other medical imaging tasks.'},\n",
       "  {'title': 'CheXseen: Unseen Disease Detection for Deep Learning Interpretation of Chest X-rays',\n",
       "   'publication date': '2021/3/8',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2103.04590',\n",
       "   'description': 'We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as\" no disease\". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.'},\n",
       "  {'title': 'VisualCheXbert: Addressing the Discrepancy Between Radiology Report Labels and Image Labels',\n",
       "   'publication date': '2021/2/23',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2102.11467',\n",
       "   'description': 'Automatic extraction of medical conditions from free-text radiology reports is critical for supervising computer vision models to interpret medical images. In this work, we show that radiologists labeling reports significantly disagree with radiologists labeling corresponding chest X-ray images, which reduces the quality of report labels as proxies for image labels. We develop and evaluate methods to produce labels from radiology reports that have better agreement with radiologists labeling images. Our best performing method, called VisualCheXbert, uses a biomedically-pretrained BERT model to directly map from a radiology report to the image labels, with a supervisory signal determined by a computer vision model trained to detect medical conditions from chest X-ray images. We find that VisualCheXbert outperforms an approach using an existing radiology report labeler by an average F1 score of 0.14 (95% CI 0.12, 0.17). We also find that VisualCheXbert better agrees with radiologists labeling chest X-ray images than do radiologists labeling the corresponding radiology reports by an average F1 score across several medical conditions of between 0.12 (95% CI 0.09, 0.15) and 0.21 (95% CI 0.18, 0.24).'},\n",
       "  {'title': 'MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation',\n",
       "   'publication date': '2021/2/21',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2102.10663',\n",
       "   'description': 'Self-supervised contrastive learning between pairs of multiple views of the same image has been shown to successfully leverage unlabeled data to produce meaningful visual representations for both natural and medical images. However, there has been limited work on determining how to select pairs for medical images, where availability of patient metadata can be leveraged to improve representations. In this work, we develop a method to select positive pairs coming from views of possibly different images through the use of patient metadata. We compare strategies for selecting positive pairs for chest X-ray interpretation including requiring them to be from the same patient, imaging study or laterality. We evaluate downstream task performance by fine-tuning the linear layer on 1% of the labeled dataset for pleural effusion classification. Our best performing positive pair selection strategy, which involves using images from the same patient from the same study across all lateralities, achieves a performance increase of 3.4% and 14.4% in mean AUC from both a previous contrastive method and ImageNet pretrained baseline respectively. Our controlled experiments show that the keys to improving downstream performance on disease classification are (1) using patient metadata to appropriately create positive pairs from different images with the same underlying pathologies, and (2) maximizing the number of different images used in query pairing. In addition, we explore leveraging patient metadata to select hard negative pairs for contrastive learning, but do not find improvement over baselines that do not use metadata. Our method is broadly …'},\n",
       "  {'title': 'CheXseg: Combining Expert Annotations with DNN-generated Saliency Maps for X-ray Segmentation',\n",
       "   'publication date': '2021/2/21',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2102.10484',\n",
       "   'description': 'Medical image segmentation models are typically supervised by expert annotations at the pixel-level, which can be expensive to acquire. In this work, we propose a method that combines the high quality of pixel-level expert annotations with the scale of coarse DNN-generated saliency maps for training multi-label semantic segmentation models. We demonstrate the application of our semi-supervised method, which we call CheXseg, on multi-label chest x-ray interpretation. We find that CheXseg improves upon the performance (mIoU) of fully-supervised methods that use only pixel-level expert annotations by 13.4% and weakly-supervised methods that use only DNN-generated saliency maps by 91.2%. Furthermore, we implement a semi-supervised method using knowledge distillation and find that though it is outperformed by CheXseg, it exceeds the performance (mIoU) of the best fully-supervised method by 4.83%. Our best method is able to match radiologist agreement on three out of ten pathologies and reduces the overall performance gap by 71.6% as compared to weakly-supervised methods.'},\n",
       "  {'title': 'CheXternal: Generalization of Deep Learning Models for Chest X-ray Interpretation to Photos of Chest X-rays and External Clinical Settings',\n",
       "   'publication date': '2021/2/17',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2102.08660',\n",
       "   'description': 'Recent advances in training deep learning models have demonstrated the potential to provide accurate chest X-ray interpretation and increase access to radiology expertise. However, poor generalization due to data distribution shifts in clinical settings is a key barrier to implementation. In this study, we measured the diagnostic performance for 8 different chest X-ray models when applied to (1) smartphone photos of chest X-rays and (2) external datasets without any finetuning. All models were developed by different groups and submitted to the CheXpert challenge, and re-applied to test datasets without further tuning. We found that (1) on photos of chest X-rays, all 8 models experienced a statistically significant drop in task performance, but only 3 performed significantly worse than radiologists on average, and (2) on the external set, none of the models performed statistically significantly worse than radiologists, and five models performed statistically significantly better than radiologists. Our results demonstrate that some chest X-ray models, under clinically relevant distribution shifts, were comparable to radiologists while other models were not. Future work should investigate aspects of model training procedures and dataset collection that influence generalization in the presence of data distribution shifts.'},\n",
       "  {'title': 'Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation',\n",
       "   'publication date': '2021/2/8',\n",
       "   'pdf link': 'https://openreview.net/pdf?id=Hp5gFnDz2Bm',\n",
       "   'description': 'Self-supervised contrastive learning between pairs of multiple views of the same image has been shown to successfully leverage unlabeled data to produce meaningful visual representations for both natural and medical images. However, there has been limited work on determining how to select pairs for medical images, where availability of patient metadata can be leveraged to improve representations. In this work, we develop a method to select positive pairs coming from views of possibly different images through the use of patient metadata. We compare strategies for selecting positive pairs for chest X-ray interpretation including requiring them to be from the same patient, imaging study or laterality. We evaluate downstream task performance by fine-tuning the linear layer on 1% of the labeled dataset for pleural effusion classification. Our best performing positive pair selection strategy, which involves using images from the same patient from the same study across all lateralities, achieves a performance increase of 3.4% and 14.4% in mean AUC from both a previous contrastive method and ImageNet pretrained baseline respectively. Our controlled experiments show that the keys to improving downstream performance on disease classification are (1) using patient metadata to appropriately create positive pairs from different images with the same underlying pathologies, and (2) maximizing the number of different images used in query pairing. In addition, we explore leveraging patient metadata to select hard negative pairs for contrastive learning, but do not find improvement over baselines that do not use metadata. Our method is broadly …'},\n",
       "  {'title': 'Unseen Disease Detection for Deep Learning Interpretation of Chest X-rays',\n",
       "   'publication date': '2021/2/8',\n",
       "   'pdf link': 'https://openreview.net/pdf?id=i-zxSlqneRu',\n",
       "   'description': \"We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) asno disease''. Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.\"},\n",
       "  {'title': 'MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation',\n",
       "   'publication date': '2021/2',\n",
       "   'pdf link': 'no link',\n",
       "   'description': 'Self-supervised contrastive learning between pairs of multiple views of the same image has been shown to successfully leverage unlabeled data to produce meaningful visual representations for both natural and medical images. However, there has been limited work on determining how to select pairs for medical images, where availability of patient metadata can be leveraged to improve representations. In this work, we develop a method to select positive pairs coming from views of possibly different images through the use of patient metadata. We compare strategies for selecting positive pairs for chest X-ray interpretation including requiring them to be from the same patient, imaging study or laterality. We evaluate downstream task performance by fine-tuning the linear layer on 1% of the labeled dataset for pleural effusion classification. Our best performing positive pair selection strategy, which involves using …'},\n",
       "  {'title': 'Real-time neural text-to-speech',\n",
       "   'publication date': '2021/1/28',\n",
       "   'pdf link': 'https://patentimages.storage.googleapis.com/be/ee/49/b6ffb4184351f5/US20210027762A1.pdf',\n",
       "   'description': 'Embodiments of a production-quality text-to-speech (TTS) system constructed from deep neural networks are described. System embodiments comprise five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For embodiments of the segmentation model, phoneme boundary detection was performed with deep neural networks using Connectionist Temporal Classification (CTC) loss. For embodiments of the audio synthesis model, a variant of WaveNet was created that requires fewer parameters and trains faster than the original. By using a neural network for each component, system embodiments are simpler and more flexible than traditional TTS systems, where each component requires laborious feature engineering and …'},\n",
       "  {'title': 'CheXtransfer: Performance and Parameter Efficiency of ImageNet Models for Chest X-Ray Interpretation',\n",
       "   'publication date': '2021/1/18',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2101.06871',\n",
       "   'description': 'Deep learning methods for chest X-ray interpretation typically rely on pretrained models developed for ImageNet. This paradigm assumes that better ImageNet architectures perform better on chest X-ray tasks and that ImageNet-pretrained weights provide a performance boost over random initialization. In this work, we compare the transfer performance and parameter efficiency of 16 popular convolutional architectures on a large chest X-ray dataset (CheXpert) to investigate these assumptions. First, we find no relationship between ImageNet performance and CheXpert performance for both models without pretraining and models with pretraining. Second, we find that, for models without pretraining, the choice of model family influences performance more than size within a family for medical imaging tasks. Third, we observe that ImageNet pretraining yields a statistically significant boost in performance across architectures, with a higher boost for smaller architectures. Fourth, we examine whether ImageNet architectures are unnecessarily large for CheXpert by truncating final blocks from pretrained models, and find that we can make models 3.25 x more parameter-efficient on average without a statistically significant drop in performance. Our work contributes new experimental evidence about the relation of ImageNet to chest x-ray interpretation performance.'},\n",
       "  {'title': 'Deep learning saliency maps do not accurately highlight diagnostically relevant regions for medical image interpretation',\n",
       "   'publication date': '2021/1/1',\n",
       "   'pdf link': 'https://www.medrxiv.org/content/medrxiv/early/2021/03/02/2021.02.28.21252634.full.pdf',\n",
       "   'description': 'Deep learning has enabled automated medical image interpretation at a level often surpassing that of practicing medical experts. However, many clinical practices have cited a lack of model interpretability as reason to delay the use of \"black-box\" deep neural networks in clinical workflows. Saliency maps, which \"explain\" a model9s decision by producing heat maps that highlight the areas of the medical image that influence model prediction, are often presented to clinicians as an aid in diagnostic decision-making. In this work, we demonstrate that the most commonly used saliency map generating method, Grad-CAM, results in low performance for 10 pathologies on chest X-rays. We examined under what clinical conditions saliency maps might be more dangerous to use compared to human experts, and found that Grad-CAM performs worse for pathologies that had multiple instances, were smaller in size, and had shapes that were more complex. Moreover, we showed that model confidence was positively correlated with Grad-CAM localization performance, suggesting that saliency maps were safer for clinicians to use as a decision aid when the model had made a positive prediction with high confidence. Our work demonstrates that several important limitations of interpretability techniques for medical imaging must be addressed before use in clinical workflows.'},\n",
       "  {'title': 'Data augmentation with Mobius transformations',\n",
       "   'publication date': '2020/12/22',\n",
       "   'pdf link': 'https://iopscience.iop.org/article/10.1088/2632-2153/abd615/pdf',\n",
       "   'description': 'Data augmentation has led to substantial improvements in the performance and generalization of deep models, and remain a highly adaptable method to evolving model architectures and varying amounts of data---in particular, extremely scarce amounts of available training data. In this paper, we present a novel method of applying Mobius transformations to augment input images during training. Mobius transformations are bijective conformal maps that generalize image translation to operate over complex inversion in pixel space. As a result, Mobius transformations can operate on the sample level and preserve data labels. We show that the inclusion of Mobius transformations during training enables improved generalization over prior sample-level data augmentation techniques such as cutout and standard crop-and-flip transformations, most notably in low data regimes.'},\n",
       "  {'title': 'Systems and methods for real-time neural text-to-speech',\n",
       "   'publication date': '2020/12/22',\n",
       "   'pdf link': 'https://patentimages.storage.googleapis.com/6b/6d/34/d9b17f792d9a9b/US10872598.pdf',\n",
       "   'description': 'Embodiments of a production-quality text-to-speech (TTS) system constructed from deep neural networks are described. System embodiments comprise five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For embodiments of the segmentation model, phoneme boundary detection was performed with deep neural networks using Connectionist Temporal Classification (CTC) loss. For embodiments of the audio synthesis model, a variant of WaveNet was created that requires fewer parameters and trains faster than the original. By using a neural network for each component, system embodiments are simpler and more flexible than traditional TTS systems, where each component requires laborious feature engineering and …'},\n",
       "  {'title': 'Upscaling FLUXNET-CH4: Data-driven model performance, predictors, and regional to global methane emission estimates for freshwater wetlands',\n",
       "   'publication date': '2020/12/9',\n",
       "   'pdf link': 'no link',\n",
       "   'description': 'no description'},\n",
       "  {'title': 'Incorporating machine learning and social determinants of health indicators into prospective risk adjustment for health plan payments',\n",
       "   'publication date': '2020/12',\n",
       "   'pdf link': 'https://link.springer.com/content/pdf/10.1186/s12889-020-08735-0.pdf',\n",
       "   'description': 'Risk adjustment models are employed to prevent adverse selection, anticipate budgetary reserve needs, and offer care management services to high-risk individuals. We aimed to address two unknowns about risk adjustment: whether machine learning (ML) and inclusion of social determinants of health (SDH) indicators improve prospective risk adjustment for health plan payments.'},\n",
       "  {'title': 'AppendiXNet: Deep Learning for Diagnosis of Appendicitis from A Small Dataset of CT Exams Using Video Pretraining',\n",
       "   'publication date': '2020/12/1',\n",
       "   'pdf link': 'no link',\n",
       "   'description': 'The development of deep learning algorithms for complex tasks in digital medicine has relied on the availability of large labeled training datasets, usually containing hundreds of thousands of examples. The purpose of this study was to develop a 3D deep learning model, AppendiXNet, to detect appendicitis, one of the most common life-threatening abdominal emergencies, using a small training dataset of less than 500 training CT exams. We explored whether pretraining the model on a large collection of natural videos would improve the performance of the model over training the model from scratch. AppendiXNet was pretrained on a large collection of YouTube videos called Kinetics, consisting of approximately 500,000 video clips and annotated for one of 600 human action classes, and then fine-tuned on a small dataset of 438 CT scans annotated for appendicitis. We found that pretraining the 3D model on …'},\n",
       "  {'title': 'CheXphoto: 10,000+ Photos and Transformations of Chest X-rays for Benchmarking Deep Learning Robustness',\n",
       "   'publication date': '2020/11/23',\n",
       "   'pdf link': 'http://proceedings.mlr.press/v136/phillips20a/phillips20a.pdf',\n",
       "   'description': 'Clinical deployment of deep learning algorithms for chest x-ray interpretation requires a solution that can integrate into the vast spectrum of clinical workflows across the world. An appealing approach to scaled deployment is to leverage the ubiquity of smartphones by capturing photos of x-rays to share with clinicians using messaging services like WhatsApp. However, the application of chest x-ray algorithms to photos of chest x-rays requires reliable classification in the presence of artifacts not typically encountered in digital x-rays used to train machine learning models. We introduce CheXphoto, a dataset of smartphone photos and synthetic photographic transformations of chest x-rays sampled from the CheXpert dataset. To generate CheXphoto we (1) automatically and manually captured photos of digital x-rays under different settings, and (2) generated synthetic transformations of digital x-rays targeted to make them look like photos of digital x-rays and x-ray films. We release this dataset as a resource for testing and improving the robustness of deep learning algorithms for automated chest x-ray interpretation on smartphone photos of chest x-rays.'},\n",
       "  {'title': 'Ngboost: Natural gradient boosting for probabilistic prediction',\n",
       "   'publication date': '2020/11/21',\n",
       "   'pdf link': 'http://proceedings.mlr.press/v119/duan20a/duan20a.pdf',\n",
       "   'description': 'We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression models output a full probability distribution over the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation-crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost matches or exceeds the performance of existing methods for probabilistic prediction while offering additional benefits in flexibility, scalability, and usability. An open-source implementation is available at github. com/stanfordmlgroup/ngboost.'},\n",
       "  {'title': 'OGNet: Towards a Global Oil and Gas Infrastructure Database using Deep Learning on Remotely Sensed Imagery',\n",
       "   'publication date': '2020/11/14',\n",
       "   'pdf link': 'https://arxiv.org/pdf/2011.07227',\n",
       "   'description': 'At least a quarter of the warming that the Earth is experiencing today is due to anthropogenic methane emissions. There are multiple satellites in orbit and planned for launch in the next few years which can detect and quantify these emissions; however, to attribute methane emissions to their sources on the ground, a comprehensive database of the locations and characteristics of emission sources worldwide is essential. In this work, we develop deep learning algorithms that leverage freely available high-resolution aerial imagery to automatically detect oil and gas infrastructure, one of the largest contributors to global methane emissions. We use the best algorithm, which we call OGNet, together with expert review to identify the locations of oil refineries and petroleum terminals in the US We show that OGNet detects many facilities which are not present in four standard public datasets of oil and gas infrastructure. All detected facilities are associated with characteristics known to contribute to methane emissions, including the infrastructure type and the number of storage tanks. The data curated and produced in this study is freely available at this http URL.'}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic extraction of medical conditions from free-text radiology reports is critical for supervising computer vision models to interpret medical images. In this work, we show that radiologists labeling reports significantly disagree with radiologists labeling corresponding chest X-ray images, which reduces the quality of report labels as proxies for image labels. We develop and evaluate methods to produce labels from radiology reports that have better agreement with radiologists labeling images. Our best performing method, called VisualCheXbert, uses a biomedically-pretrained BERT model to directly map from a radiology report to the image labels, with a supervisory signal determined by a computer vision model trained to detect medical conditions from chest X-ray images. We find that VisualCheXbert outperforms an approach using an existing radiology report labeler by an average F1 score of 0.14 (95% CI 0.12, 0.17). We also find that VisualCheXbert better agrees with radiologists labeling chest X-ray images than do radiologists labeling the corresponding radiology reports by an average F1 score across several medical conditions of between 0.12 (95% CI 0.09, 0.15) and 0.21 (95% CI 0.18, 0.24).\n",
      "\n",
      "Automatic extraction of medical conditions from free-text radiology reports is critical for supervising computer vision models to interpret medical images. We develop and evaluate methods to produce labels from radiology reports that have better agreement with radiologists labeling images. We find that VisualCheXbert outperforms an approach using an existing radiology report labeler by an average F1 score of 0.14 (95% CI 0.12, 0.17).\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "for stoo in profile['studies']:\n",
    "    formatted_desc = re.sub('[^a-zA-Z]', ' ', stoo['description'] )\n",
    "    formatted_desc = re.sub(r'\\s+', ' ', formatted_desc)\n",
    "    formatted_desc = formatted_desc.lower()\n",
    "    sentence_list = nltk.sent_tokenize(stoo['description'])\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_desc):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "                \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)            \n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    print(stoo['description'])\n",
    "    print('')\n",
    "    print(summary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scholar.google.com/citations?hl=en&user=75udJTAAAAAJ'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getResearcherURL(name, university=''):\n",
    "    if name == '':\n",
    "        return 'input name'\n",
    "    if university != '':\n",
    "        t_name = name + ' ' + university\n",
    "        \n",
    "    search_url = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors='\n",
    "    t_name = name.split(' ')\n",
    "    \n",
    "    for i in t_name:\n",
    "        search_url += i + '+'\n",
    "        \n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text,'html')\n",
    "    summ = soup.findAll(\"div\", {\"class\": \"gsc_1usr\"})\n",
    "    \n",
    "    for i in summ:\n",
    "        if university in i.text and name in i.text:\n",
    "            href = i.find_all('a', href=True)[0]['href']\n",
    "            return (\"scholar.google.com\"+href)\n",
    "    return 'no valid researcher found'\n",
    "        \n",
    "getResearcherURL('Andrew Garcia', 'University of Florida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.espn.com/nba/story/_/id/31102059/nba-power-rankings-lebron-james-injury-means-our-latest-update-plus-new-no-1-rises\n",
      "https://www.cbssports.com/nba/news/nba-power-rankings-lakers-tumble-after-lebron-james-injury-nets-stay-no-1-as-trade-deadline-nears/\n",
      "https://sports.yahoo.com/lebron-james-absence-exposes-lakers-deeper-issue-182858005.html\n",
      "http://www.lebronjames.com/\n",
      "https://en.wikipedia.org/wiki/LeBron_James\n",
      "https://www.basketball-reference.com/players/j/jamesle01.html\n",
      "https://twitter.com/KingJames?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\n",
      "https://twitter.com/KingJames/status/1373424404923785218?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet\n",
      "https://twitter.com/KingJames/status/1373008005013245953?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet\n",
      "https://twitter.com/KingJames/status/1372073046228410368?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet\n"
     ]
    }
   ],
   "source": [
    "for j in search('LeBron James', tld='com', lang='en', start=0, num=20, stop=10, pause=2): \n",
    "    print(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time \n",
    "\n",
    "def getResearcherURL(name, university=''):\n",
    "    if name == '':\n",
    "        return 'input name'\n",
    "    if university != '':\n",
    "        t_name = name + ' ' + university\n",
    "        \n",
    "    search_url = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors='\n",
    "    t_name = name.split(' ')\n",
    "    \n",
    "    for i in t_name:\n",
    "        search_url += i + '+'\n",
    "        \n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text,'html')\n",
    "    summ = soup.findAll(\"div\", {\"class\": \"gsc_1usr\"})\n",
    "    \n",
    "    for i in summ:\n",
    "        if university in i.text and name in i.text:\n",
    "            href = i.find_all('a', href=True)[0]['href']\n",
    "            return (\"scholar.google.com\"+href)\n",
    "    return 'no valid researcher found'\n",
    "\n",
    "def getResearcherProfile(url):\n",
    "    driver = webdriver.Chrome('//Users/shardulkothapalli/Desktop/SCHOOL/6.spring2021/cs3312/StudyFind-0320/chromedriver')\n",
    "    url += '&view_op=list_works&sortby=pubdate'\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source,'html')\n",
    "\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html')\n",
    "\n",
    "\n",
    "    prof = soup.findAll(\"div\", {\"id\": \"gsc_prf_i\"})\n",
    "    tags = soup.findAll(\"a\", {\"class\": \"gsc_prf_inta gs_ibl\"})\n",
    "    prof_info = []\n",
    "\n",
    "    for row in prof[0].find_all(\"div\")[:2]:\n",
    "        prof_info.append(row.text)\n",
    "    temp_tags = []\n",
    "    for tag in tags:\n",
    "        temp_tags.append(tag.text)\n",
    "    prof_info.append(temp_tags)\n",
    "    prof_pic = driver.find_element_by_xpath('//*[@id=\"gsc_prf_pup-img\"]')\n",
    "    prof_pic = prof_pic.get_attribute('src')\n",
    "\n",
    "    studies = soup.findAll(\"tbody\", {\"id\": \"gsc_a_b\"})\n",
    "    count = 1\n",
    "\n",
    "    stoodies = []\n",
    "    for row in studies[0].find_all(\"tr\", {'class': 'gsc_a_tr'}):\n",
    "        xp = '//*[@id=\"gsc_a_b\"]/tr[' + str(count) + ']/td[1]/a'\n",
    "        elem = driver.find_element_by_xpath(xp)\n",
    "        elem.click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_title\"]/a')\n",
    "            title = title.text\n",
    "        except:\n",
    "            title = 'no title'\n",
    "\n",
    "        try:\n",
    "            pubdate = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_table\"]/div[2]/div[2]')\n",
    "            pubdate = pubdate.text\n",
    "        except:\n",
    "            pubdate = 'no date'\n",
    "\n",
    "        try:\n",
    "            pdflink = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_title_gg\"]/div/a')\n",
    "            pdflink = pdflink.get_attribute('href')\n",
    "        except:\n",
    "            pdflink = 'no link'\n",
    "\n",
    "        try:\n",
    "            desc = driver.find_element_by_xpath('//*[@id=\"gsc_vcd_descr\"]/div/div')\n",
    "            desc = desc.text\n",
    "        except:\n",
    "            desc = 'no description'\n",
    "\n",
    "        study = {'title': title, 'publication date': pubdate, 'pdf link': pdflink, 'description': desc}\n",
    "        stoodies.append(study)\n",
    "\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"gs_md_cita-d-x\"]/span[1]')\n",
    "        elem.click()\n",
    "\n",
    "        count += 1    \n",
    "    driver.quit()\n",
    "    profile = {'name': prof_info[0], 'organization': prof_info[1], 'topics': prof_info[2], 'profile pic': prof_pic,\n",
    "               'studies': stoodies}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
